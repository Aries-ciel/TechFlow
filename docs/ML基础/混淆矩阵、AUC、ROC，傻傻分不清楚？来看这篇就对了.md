今天是**机器学习专题的第18篇**文章，我们来看看机器学习领域当中，非常重要的其他几个指标。

## 混淆矩阵

在上一篇文章当中，我们在介绍召回率、准确率这些概念之前，先讲了**TP、FP、FN、和FP**这几个值。我们再来简单地回顾一下，我们不能死记硬背这几个指标，否则很容易搞错，并且还容易搞混。我们需要从英文入手来理解，其中的T表示真，可以理解成**预测正确**，F表示假，也就是**预测错误**。而P和N表示positive和negative，也就是阴和阳，或者是0和1，也就是两个不同的类别。

既然是两个类别，那么显然说明了我们的这些指标针对的是二分类的场景，也是机器学习当中最常见的场景。


混淆矩阵其实本质上就是将这四个值展示在一个表格当中，这样方便我们观察结果做出分析。

我们举个例子：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9quu5dhmj30b003a0sp.jpg)

假设某一个模型的预测结果的混淆矩阵是这样，我们从上面展示的数据当中很容易就分析出，我们**预测的错误主要发生在49这一格**，也就是假阴性这一格。也就是说模型将大量的阳性样本预测成了阴性，说明模型的阈值设置得过高，我们可以尝试降低阈值来提升扩大召回。

反之，如果假阳性的样本太多，则说明模型的阈值过低，将大量阴性的样本预测成了阳性。我们想要提升模型的效果，可以考虑提升一下模型分类的阈值。

那**如果假阳和假阴都很多该怎么办？**

这种情况也很多，一般情况下是由于模型**没有完全收敛**，或者是模型不够强大。比如特征过多，特征当中很多隐藏的信息没有能够学习到。这个时候可以考虑使用更加复杂的模型，比如神经网络或者是XGboost这种较为强力的模型。如果模型本身已经足够复杂，那么可能是**训练的时候的样本数量不够多**，导致模型的能力无法完全发挥，这个时候可以考虑增加一些样本。

理解了混淆矩阵的概念和用途之后，我们就可以进一步来看ROC了。

## ROC

ROC的英文是**receiver operating characteristic curve**，翻译过来是接受者操作特征曲线，这是一个从信号系统学科当中迁移过来的一个概念。老实讲我不太了解信号系统，不太清楚它原本的含义，但是在机器学习当中，它是反应TPR和FPR的曲线。

标一下关键点，**TPR和FPR以及曲线**。这里的TRP就是True Positive Rate，也就是真阳率，这里的FPR是假阳率。

所谓的真阳率也就是召回率，也就是**所有阳性样本当中被我们预测成阳性的比例**。

$$TPR(recall) = \frac{TP}{TP+FN}$$

FPR自然就是False Positive Rate，也就是假阳率，是**所有阴性样本当中被预测成阳性的比例**。分母显然是FP，分子是FP + TN。

$$FPR = \frac{FP}{FP+TN}$$

我**建议大家不要把TPR理解成recall**，虽然它的确就是recall但是如果你记成recall的话，会增加记忆成本。横轴和纵轴记成FPR和TPR比较好记。

所以ROC曲线就是横轴是FPR纵轴是TPR的曲线，大概是下面这个样子。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9qx790uhj315x0u040l.jpg)

## AUC

理解了ROC之后，AUC就容易了。因为AUC完全源于ROC，它的英文是**Area under curve**，也就是ROC曲线当中曲形的面积。

那么，这个ROC是怎么算出来的呢？

我们来举一个例子，假设我们现在有一系列预测结果：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9qxrkotrj30u2027glp.jpg)

我们列一下这个模型的混淆矩阵：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9qxytzotj30b003aq2w.jpg)

我们代入算一下FPR和TPR，可以得到TPR是3 / (3 + 2) = 0.6，对应的FPR是1 / (1 + 4) = 0.2。

我们把这个点代入ROC曲线，可以得到：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9qyvzhklj318d0u041h.jpg)

看起来像是那么回事了，但还是有些怪怪的，这**看起来也不像是一个曲线呀**。这是因为我们模型预测的结果直接拿的是01值，对于一些硬分类器，比如SVM和贝叶斯，0就是0，1就是1，我们得到的就是这样一个折线图。但如果是一些根据阈值划分结果的软分类器，比如LR、GBDT等，我们得到的就是一个浮点值，我们调整阈值就会得到不同的结果，就会更加像是曲线。

我们还用刚才的样本举例：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9r0fy9yqj30u2027glq.jpg)

这次的结果是一个浮点值，结果就不一样了。由于预测结果是一个浮点值，我们**设置不同的阈值就会得到不同的混淆矩阵**。

比如，如果我们设置阈值为0.5，得到的混淆矩阵如下：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9r0ocrw6j30b003aq2w.jpg)

这样算出来的TPR和FPR分别是0.8，0.4。如果我们进一步放宽阈值，可以提升召回，也就是提升TPR，但与此同时FPR也会提升。比如如果我们把阈值放宽到0.2，我们可以识别出所有的正例，但是同样的，FPR也会上升：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9r12ld5zj30b003aq2w.jpg)

根据上面这个混淆矩阵计算得出的结果TPR是1.0，FPR是0.6。也就是说我们**选择不同的阈值会得到不同的TPR，和FPR**。如果样本较少的话，画出来的ROC可能是**锯齿形**：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9r1b5cs4j30u00u07ct.jpg)


当样本的数量足够多之后，锯齿会变得越来越光滑，我们可以再用上一些平滑的方法，可以得到一个相对光滑的曲线，就变成了上面那张图：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9r9s8h2bj315x0u0ac6.jpg)

## 深度理解

现在我们搞清楚了AUC的概念，AUC就是ROC曲线围成的图形面积。而ROC曲线上每一个点都是通过不同的阈值计算得到的点。

我们结合一下AUC的图像以及上面的例子来深度理解一下这个概念，对于AUC曲线而言，我们发现它是**单调递增**的。也就是说FPR越大，对应的TPR也就越大。这个是比较直观的，因为**FPR越大，说明我们把更多的样本预测成了正例，那么显然TPR也就越大**。也就是说我们召回的正样本变多了，比例也就变多了。

当FPR=1的时候TPR也等于1，这个点表明我们把所有的样本都预测成了正例。显然在这种情况下，所有的正例都被预测对了，TPR自然就是1。我们再来看另外一个极值点，也就是FPR等于0的点。

FPR等于0表明了假阴率为0，也就是说没有一个负样本被预测错，也就对应着模型预测为正例的样本数非常少。所以FPR这个点对应的TPR越高，往往说明模型的效果越好。

我们理解了AUC的概念之后，免不了问一个问题，AUC这个值究竟代表了什么呢，能够反映什么结果呢？

我们来看下面这张图：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9r2p8iqkj31a30u00v3.jpg)

下面这张图中的绿线围成的面积明显大于粉线，也就是AUC1 > AUC2。从这张图我们可以看出，AUC越大，说明曲线围成的面积越大，如果我们选择0-1当中去一个点做垂线，可以得到相同FPR下，通常AUC越大的，对应的TPR也越大（有反例，见下图）。

**TPR越大说明模型可以在分错同样数量负样本的情况下预测正确更多的正样本，这代表了模型区分正负样本的能力**。

为什么要比较AUC而不是设定一个阈值比较TPR呢？

因为有些时候模型的情况比较复杂，比如下面这张图：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9r3ujzgfj31a10u0dih.jpg)

在p点以前紫色模型的效果明显更好，但是p点之后就是粉红色的模型更好了。如果**只凭单个点的情况，我们很难反应模型整体的能力**。所以用AUC可以衡量模型**整体上**区分正负样本的能力。

最后我们来思考一个问题，AUC最坏的情况是多少？会是0吗？

错了，AUC**最坏的情况是0.5**。因为如果是随机猜测正负例，那么我们猜测正确的正例数量应该永远占当前猜测数量的0.5，在这种情况下TPR和FPR一直相等，也就是我们画出来的是一条直线，比如下图：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge9r5qqlfrj30a207ywep.jpg)

如果算出来的AUC小于0.5怎么办？说明模型可能**学习到了样本和特征之间负相关的关系**，我们可以试着调换一下0和1两个类别，算出来的AUC应该能变成0.5以上。

## 总结

在前面的文章当中我们曾经说过，在机器学习的使用场景当中，我们**往往更加看重正例**。比如广告的点击率预测、搜索排序、推荐等等这些场景下，我们更加关注用户点击行为的发生和预测准确情况，而不太关心没有点击是否预测准确。在这些场景当中，我们衡量精确度或者是召回其实不是特别重要，尤其这种涉及排序、摆放位置调整的场景，我们**更加在意模型是否能够把高质量的内容给出一个高的预测分**，让它能够排在前面，让用户优先看到。这个时候往往AUC更加能够说明模型的能力。

也因此，相比于精确度、准确度和召回率，在实际的工业应用场景当中，我们可能**使用AUC更多一些**。当然这并非是说其他概念不重要，这主要还是应用场景决定的。既然应用场景决定了使用AUC的范围很广，那么当我们去应聘岗位的时候，问到AUC的可能性就很高，尤其是考察候选人基础能力的时候。如果被问到，**光理解它是什么意思是不够的**，我们还需要掌握它的应用场景，它的前因后果，甚至能够进行发散思考一些之前没有想过的问题。

希望大家都能有所收获，原创不易，厚颜求个**赞和转发**，让我们为了成为更优秀的自己而努力。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf3ak9b69aj3076076dgg.jpg)